{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Libraries upload"
      ],
      "metadata": {
        "id": "ym1m2Z6x9edg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jellyfish\n",
        "!pip install SPARQLWrapper"
      ],
      "metadata": {
        "id": "5bKSiZiw--BB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from jellyfish import jaro_similarity as jaro\n",
        "from itertools import combinations\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON"
      ],
      "metadata": {
        "id": "lOnK3nQL9AWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset read"
      ],
      "metadata": {
        "id": "W-jMPU8i9zkJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZDDKu8M8vob"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('with_extracted_pers_orgs.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additional cleaning of data: limiting news by a 10 year time frame"
      ],
      "metadata": {
        "id": "8cSUAqhk92ya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ten_years = df[df['date'] > '2009/12/13']"
      ],
      "metadata": {
        "id": "aVUrvWyi9O2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving extracted entities into a better format (on the previous step it simply saved the vocab of entities straight into the dataframe)"
      ],
      "metadata": {
        "id": "UK77f4Dk98-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clean_person = []\n",
        "clean_organization = []\n",
        "\n",
        "for index, row in ten_years.iterrows():\n",
        "    current = row\n",
        "    current['persons'] = current['persons'].replace('{', '').replace(',', '').replace('\"', '').replace('}', '')\n",
        "    current['organizations'] = current['organizations'].replace('{', '').replace(',', '').replace('\"', '').replace('}', '')\n",
        "\n",
        "    pattern = re.compile(r'[^\\']+')\n",
        "\n",
        "    matches = re.findall(pattern, current['persons'])\n",
        "    matches = [m for m in matches if m != ' ' and m != 'set()']\n",
        "    clean_person.append('@'.join(matches))\n",
        "\n",
        "    matches = re.findall(pattern, current['organizations'])\n",
        "    matches = [m for m in matches if m != ' ' and m != 'set()']\n",
        "    clean_organization.append('@'.join(matches))"
      ],
      "metadata": {
        "id": "ON1z1Idb9SF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the edited dataframe"
      ],
      "metadata": {
        "id": "wVw-GgRX-Q4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ten_years['persons'] = clean_person\n",
        "ten_years['organizations'] = clean_organization\n",
        "ten_years.to_csv('ten_years_with_good_format.csv', index = False)"
      ],
      "metadata": {
        "id": "Fq6SYGbs9aKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the dataframe"
      ],
      "metadata": {
        "id": "xZedONGB-cgx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ten_years"
      ],
      "metadata": {
        "id": "rpIcbOZK-aQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function for testing if a pair of keywords is too similar and needs to be removed (control over spacy's issues with cases in Russian language)"
      ],
      "metadata": {
        "id": "zmgczfL1-6FC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_similarity(list_of_pairs):\n",
        "    removed = set()\n",
        "    to_add = set()\n",
        "\n",
        "    for pair in list_of_pairs:\n",
        "        el1 = pair[0]\n",
        "        el2 = pair[1]\n",
        "\n",
        "        #check length of the element - if length lower or equal to 3, then it is\n",
        "        #likely to be an abbreviation, thus jaro similarity will be too sensitive\n",
        "        if len(el1) > 3:\n",
        "            if len(el2) > 3:\n",
        "                #if jaro similarity is higher than 0.9, then keywords\n",
        "                #are considered the same\n",
        "                if jaro(el1, el2) >= 0.9:\n",
        "                    #saving the keyword with the shortest length\n",
        "                    if len(el1) < len(el2):\n",
        "                        to_add.add(el1)\n",
        "                        removed.add(el2)\n",
        "                        if el2 in to_add:\n",
        "                            to_add.remove(el2)\n",
        "                    else:\n",
        "                        to_add.add(el2)\n",
        "                        removed.add(el1)\n",
        "                        if el1 in to_add:\n",
        "                            to_add.remove(el1)\n",
        "                else:\n",
        "                    if el1 not in removed:\n",
        "                        to_add.add(el1)\n",
        "                    if el2 not in removed:\n",
        "                        to_add.add(el2)\n",
        "            else:\n",
        "                if el1 not in removed:\n",
        "                    to_add.add(el1)\n",
        "                if el2 not in removed:\n",
        "                    to_add.add(el2)\n",
        "        else:\n",
        "            if el1 not in removed:\n",
        "                to_add.add(el1)\n",
        "            if el2 not in removed:\n",
        "                to_add.add(el2)\n",
        "\n",
        "    return to_add"
      ],
      "metadata": {
        "id": "Z7k6qVsj-exf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving all key terms and their absolute number of occurences in the dataset into a special dictionary"
      ],
      "metadata": {
        "id": "Ag7GJnT8Aiph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_terms = dict()\n",
        "\n",
        "for index, row in ten_years.iterrows():\n",
        "    orgs = row['organizations'].split('@')\n",
        "    to_check = list(combinations(orgs, 2))\n",
        "    to_add = check_similarity(to_check)\n",
        "\n",
        "    for el in to_add:\n",
        "        if el in all_terms:\n",
        "            all_terms[el][0] += 1\n",
        "        else:\n",
        "            all_terms[el] = [1, 'org']\n",
        "\n",
        "    pers = row['persons'].split('@')\n",
        "    to_check = list(combinations(pers, 2))\n",
        "    to_add = check_similarity(to_check)\n",
        "\n",
        "    for el in to_add:\n",
        "        if el in all_terms:\n",
        "            all_terms[el][0] += 1\n",
        "        else:\n",
        "            all_terms[el] = [1, 'pers']"
      ],
      "metadata": {
        "id": "qtLgSzIdAfJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_terms"
      ],
      "metadata": {
        "id": "bVQy_sV6BAc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If a term appears in the dataset more than 35 times, then it is considered significant enough to be analyzed at the later stages"
      ],
      "metadata": {
        "id": "xcNt5GS6A4Xa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_all_terms = dict()\n",
        "\n",
        "for term in all_terms:\n",
        "    #threshold 35 was taken after analyzing the outputs in all_terms\n",
        "    if all_terms[term][0] >= 35:\n",
        "        cleaned_all_terms[term] = all_terms[term]"
      ],
      "metadata": {
        "id": "kySUxr-wA0TY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Total number of keywords in the dataset"
      ],
      "metadata": {
        "id": "nD3gBfSlBRDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(cleaned_all_terms)"
      ],
      "metadata": {
        "id": "ruwnalx7BObG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function for DBpedia link retrieval"
      ],
      "metadata": {
        "id": "nQdX8mSQBxFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def return_dbpedia_link(term, term_type):\n",
        "    entity_type = ''\n",
        "\n",
        "    if term_type == 'org':\n",
        "        entity_type = '?s1 a <http://dbpedia.org/ontology/Organisation> .'\n",
        "    else:\n",
        "        entity_type = '?s1 a <http://dbpedia.org/ontology/Person> .'\n",
        "\n",
        "    sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
        "\n",
        "    #if the entity is an organization or a person entity of only one word,\n",
        "    #then proceed with the query\n",
        "    if term_type == 'org' or (term_type == 'pers' and len(term.split()) == 1):\n",
        "        query = f\"\"\"\n",
        "     define input:ifp \"IFP_OFF\"  select ?s1 as ?c1, (bif:search_excerpt (bif:vector ('{term}'), ?o1)) as ?c2, ?sc, ?rank, ?g where {{{{ select ?s1, (?sc * 3e-1) as ?sc, ?o1, (sql:rnk_scale (<LONG::IRI_RANK> (?s1))) as ?rank, ?g where\n",
        "  {{\n",
        "    quad map virtrdf:DefaultQuadMap\n",
        "    {{\n",
        "      graph ?g\n",
        "      {{\n",
        "         ?s1 ?s1textp ?o1 .\n",
        "        ?o1 bif:contains  '(\"{term}\")'  option (score ?sc)  .\n",
        "\n",
        "      }}\n",
        "     }}\n",
        "    {entity_type}\n",
        "  }}\n",
        " order by desc (?sc * 3e-1 + sql:rnk_scale (<LONG::IRI_RANK> (?s1)))  limit 5  offset 0 }}}}\n",
        "\"\"\"\n",
        "    #otherwise the term to be sent needs to be preprocessed into the appropriate\n",
        "    #format\n",
        "    else:\n",
        "        parts = term.split()\n",
        "        first_input = ''\n",
        "        for part in parts:\n",
        "            first_input += '\\'' + part + '\\'' + ', '\n",
        "\n",
        "        second_input = ''\n",
        "\n",
        "        for part in parts:\n",
        "            second_input += '\\\"' + part + '\\\"' + ' AND '\n",
        "\n",
        "        first_input = first_input.rstrip(', ')\n",
        "        second_input = second_input.rstrip(' AND ')\n",
        "\n",
        "        query = f\"\"\"\n",
        "     define input:ifp \"IFP_OFF\"  select ?s1 as ?c1, (bif:search_excerpt (bif:vector ({first_input}), ?o1)) as ?c2, ?sc, ?rank, ?g where {{{{ select ?s1, (?sc * 3e-1) as ?sc, ?o1, (sql:rnk_scale (<LONG::IRI_RANK> (?s1))) as ?rank, ?g where\n",
        "  {{\n",
        "    quad map virtrdf:DefaultQuadMap\n",
        "    {{\n",
        "      graph ?g\n",
        "      {{\n",
        "         ?s1 ?s1textp ?o1 .\n",
        "        ?o1 bif:contains  '({second_input})'  option (score ?sc)  .\n",
        "\n",
        "      }}\n",
        "     }}\n",
        "    {entity_type}\n",
        "  }}\n",
        " order by desc (?sc * 3e-1 + sql:rnk_scale (<LONG::IRI_RANK> (?s1)))  limit 1  offset 0 }}}}\n",
        "\"\"\"\n",
        "\n",
        "    sparql.setQuery(query)\n",
        "    sparql.setReturnFormat(JSON)\n",
        "    result = sparql.query().convert()\n",
        "\n",
        "    #if link was not found, return the appropriate message,\n",
        "    #otherwise return the top 1 link\n",
        "    if result['results']['bindings'] == []:\n",
        "        return 'link_not_found'\n",
        "    else:\n",
        "        return result['results']['bindings'][0]['c1']['value']"
      ],
      "metadata": {
        "id": "cOelRy-4BpOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting links from DBpedia"
      ],
      "metadata": {
        "id": "kSAl1oEJDGPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "terms_with_links = []\n",
        "\n",
        "for term in cleaned_all_terms:\n",
        "    terms_with_links.append([term, cleaned_all_terms[term][0], cleaned_all_terms[term][1],\n",
        "                             return_dbpedia_link(term, cleaned_all_terms[term][1])])"
      ],
      "metadata": {
        "id": "C2cCqaCgDAvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving all information about the terms"
      ],
      "metadata": {
        "id": "5PmJFudADNyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_terms = pd.DataFrame(terms_with_links, columns = ['term', 'occurences', 'type', 'link'])\n",
        "save_terms"
      ],
      "metadata": {
        "id": "TiL5U-h7DFvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_terms.to_csv('extracted_terms_with_links.csv', index = False)"
      ],
      "metadata": {
        "id": "Yz_rVB9TDSBw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}